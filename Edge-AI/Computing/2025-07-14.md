# 📅 2025-07-11 Paper Reading Log
> 今日目标：10 篇 | 实际完成：3 / 10  
Today’s Goal: 10 papers  
Focus: Edge AI   

# 📚 Table of Contents

1. [1. CEED: Collaborative Early Exit Neural Network Inference at the Edge](#1-ceed-collaborative-early-exit-neural-network-inference-at-the-edge)  
2. [2. Throughput Maximization of Delay-Aware DNN Inference in Edge Computing by Exploring DNN Model Partitioning and Inference Parallelism](#2-throughput-maximization-of-delay-aware-dnn-inference-in-edge-computing-by-exploring-dnn-model-partitioning-and-inference-parallelism)  
3. [3. Deep Reinforcement Learning for Containerized Edge Intelligence Inference Request Processing in IoT Edge Computing](#3-deep-reinforcement-learning-for-containerized-edge-intelligence-inference-request-processing-in-iot-edge-computing)


## 1. CEED: Collaborative Early Exit Neural Network Inference at the Edge  
- **原文链接**: https://ieeexplore.ieee.org/document/11044557   
- **作者**: Yichong Chen; Zifeng Niu; Manuel Roveri; Giuliano Casale  
- **期刊**: IEEE INFOCOM 2025 - IEEE Conference on Computer Communications  
- **标签**: #边缘计算 #早期退出神经网络 #服务质量   

> ### 摘要 
近年来，边缘的协作推理作为边缘计算的主要趋势之一受到了越来越多的关注。早期退出神经网络（EENN）体系结构通过在神经网络内利用可配置的早期退出阈值来平衡推理时间和准确性来支持这一点。这样的阈值使得能够基于置信度分数来动态地调整作业的处理等待时间。然而，大多数分布式EENN设置使用预设的置信度阈值，并假设数据到达恒定。由于边缘设备中的有限存储器容量，这种假设使系统面临潜在的数据丢失。针对该问题，该文提出了一种基于人工智能的协同EENN推理框架CEED.CEED集成了EENN预测器和丢失率预测器，可快速评估置信阈值配置和设备的作业分配。实验结果表明，CEED算法在端到端系统丢包率和EENN推理精度之间取得了较好的平衡，显著改善了现有的EENN推理方法。

---

## 背景与挑战
- 深度学习模型越来越深，边缘设备资源有限 → **推理延迟高**且**易因内存溢出丢包**。  
- **早退神经网络（EENN）** 在主干网络中插入若干中间分类器（IC），当置信度 δ ≥ 阈值 τ 时提前输出结果，牺牲少量精度换取显著加速。  
- **协作推理** 把 EENN 拆分到多层边缘设备，进一步降低单节点压力。  
- **关键难点**  
  1. 如何给各设备上的 IC 设置合适的 τ（既要保证精度，又要避免排队丢包）。  
  2. 如何为每个任务动态选择**推理链**（chain），即数据在多层设备上的实际流经路径。  
  3. 现有方法通常固定阈值、忽视内存限制，导致高丢包率。

---

## CEED 框架总览
CEED 是一套**AI 驱动的离线优化框架**，生成一个轻量级运行时策略 Π，包含：
- **置信阈值配置** τᵢ：网关 i 上所有 IC 的阈值。  
- **链分配策略** aᵢ：把任务映射到不同推理链的概率分布。

运行时，网关只需按离线生成的概率随机选链，本地无需再做重计算。

---

## 两大预测器
| 名称 | 模型 | 输入 | 输出 | 作用 |
|---|---|---|---|---|
| **EENN 预测器** | 仅解码器 Transformer | τᵢ（阈值向量） | 各 IC 的退出概率 q̂、置信分 δ̂ | 估计整体精度 |
| **丢包率预测器** | 图神经网络（GNN） | 系统拓扑、τ、链分配 | 系统丢包率 D̂ | 估计丢包风险 |

- **EENN 预测器** 用因果掩码保证后一个 IC 只能“看到”前面阈值的信息。  
- **GNN 预测器** 把排队网络抽象为图，节点 = 设备 + 有限内存区域，边 = 网络传输，支持早退、带宽、内存占用一体化建模。

---

## 优化目标
最大化奖励  
r = (加权平均置信分) − γ·max(D̂ − D*, 0)  
- 置信分 ↑ → 精度 ↑  
- D̂ ≤ D* 时不惩罚；否则按 γ 惩罚  
通过 **Adam 梯度下降** 反复迭代 τ 与链分数 s，直至收敛。

---

## 实验结果
- **测试平台**：3 层边缘节点（树莓派 + Mini PC），EENN 规模：Custom CNN、ResNet50、ResNet101。  
- **对比基线**  
  - 早退固定阈值（Exit-last、Rate-based）  
  - 强化学习 AdaEE  
  - 传统拆分 Neurosurgeon  
- **关键结论**  
  1. 在负载 ρ = 3 时，CEED 精度 **0.91**，显著高于 Rate-based（0.77）且几乎无丢包。  
  2. 在 ResNet101 大模型场景，AdaEE 动作空间爆炸，性能骤降，而 CEED 仍保持优势。  
  3. 预测器误差极低：置信分 MSE ≈ 0.01；丢包率 MAE 比 ChainNet 降低 24%。

---

## 主要贡献
1. 首个**联合优化精度、延迟、丢包率** 的协作 EENN 框架。  
2. **Transformer 预测器**：用序列建模解决阈值间复杂依赖。  
3. **GNN 预测器**：首次把早退、有限内存、带宽瓶颈统一进图模型。  
4. 物理测试床验证：在真实树莓派集群上端到端领先现有方法。

---

## 未来工作
- 研究数据分布漂移（concept drift）对阈值策略的影响。  
- 将推理链延伸至云端，形成**云-边-端**多级早退。




## 2. Throughput Maximization of Delay-Aware DNN Inference in Edge Computing by Exploring DNN Model Partitioning and Inference Parallelism  
- **原文链接**: https://ieeexplore.ieee.org/document/9606540   
- **作者**: Jing Li; Weifa Liang; Yuchen Li; Zichuan Xu; Xiaohua Jia; Song Guo  
- **期刊**: IEEE Transactions on Mobile Computing  
- **标签**: #移动边缘计算（MEC） #DNN 模型推理配置 #吞吐量最大化 #智能物联网设备 #近似和在线算法 #延迟感知 DNN 推理 #DNN 分区 #推理并行 #计算和带宽资源分配与优化 #算法设计和分析   

> ### 摘要 
移动边缘计算 (MEC) 通过将计算密集型任务转移至 MEC 网络进行处理，已成为一种极具前景的计算模式，可满足移动应用爆炸式增长的需求。深度学习的蓬勃发展为智能物联网 (IoT) 的前景注入了新的生机和活力，而边缘智能的出现则为用户提供了实时深度神经网络 (DNN) 推理服务。为了加速 MEC 网络中用户请求的 DNN 推理处理，通常可将 DNN 推理模型划分为两个相互关联的部分：一部分在请求的本地物联网设备中处理，另一部分在 MEC 网络中的云端（边缘服务器）中处理。此外，通过为请求分配多个云端线程，可以进一步加速 DNN 推理。在本文中，我们研究了一种新颖的延迟感知 DNN 推理吞吐量最大化问题，旨在通过联合探索 DNN 划分和多线程执行并行性来加速每个 DNN 推理，从而最大化所接纳的延迟感知 DNN 服务请求的数量。具体而言，我们分别考虑离线和在线请求到达设置下的问题：预先给出一组 DNN 推理请求，并且 DNN 推理请求序列在不知道未来到达的情况下逐个到达。我们首先证明所定义的问题是 NP 难的。然后，我们针对离线设置下的问题设计了一种新颖的常数近似算法。我们还针对在线设置下的问题提出了一种具有可证明竞争比的在线算法。最后，我们通过实验模拟评估了所提算法的性能。实验结果表明，所提算法具有良好的前景。

---

## 研究背景  
- **移动边缘计算（MEC）** 将云算力下沉到基站侧，为 IoT 设备提供低延迟 AI 推理服务。  
- **痛点**：IoT 设备算力有限，而边缘云节点（cloudlet）的 CPU 核心数有限（最多给单个任务开 **K** 个线程）。如何在满足每个推理请求 **硬截止延迟** 的前提下，**让系统接纳更多请求**？

---

## 系统模型  
| 对象 | 符号 | 关键参数 |
|---|---|---|
| **边缘云节点** | `n ∈ N` | 拥有 `Cn` 个 CPU 线程，带宽 `Bn` |
| **推理请求** | `ri` | 有向无环图 `Gi = (Vi, Ei)`，截止延迟 `Di`，每层算量 `f(vi,j)` |
| **决策变量** | `Vloci , Vmeci` | 本地设备/云节点分别执行的层集合 |
| **线程数** | `ki,n ∈ [1, K]` | 为 `ri` 在云节点 `n` 分配的线程数量 |
| **端到端延迟** | `dd2d = d本地 + d传输 + d云端` | 必须 ≤ `Di` |

---

## 核心问题  
**延迟敏感 DNN 推理吞吐最大化问题**  
- **离线版**：给定一批请求，最大化可接纳数量（NP-hard）。  
- **在线版**：请求按序到达、无未来信息，最大化接纳数量。

---

## 技术路线  
| 场景 | 方法 | 关键思想 | 性能保证 |
|---|---|---|---|
| **离线** | **近似算法** | 1. 为每个 `(ri, n, ki,n)` 构造 **辅助流图**<br>2. 最小割→最优划分 + 最少线程<br>3. 将问题规约到 **最大利润 GAP**，调用 `1/2+ε` 近似算法 | `1/2+ε` 近似比，`O(|R|·|N|·logK·|V|^3)` |
| **在线** | **竞争算法** | 1. 同样用最小割+二分搜索求 `kmin_i,n`<br>2. 按 **指数成本函数** 选“最空”云节点<br>3. 若成本>阈值则拒绝 | 竞争比 `O(log|N|)` |

---

## 实验验证  
- **测试规模**：1 km² 区域 100 个 cloudlet，1000 个请求，AlexNet / ResNet / VGG 等模型。  
- **结果摘要**：  
  - 离线算法比 Neurosurgeon 基线 **提升 17–24% 吞吐量**。  
  - 在线算法在 `K=10` 时比贪婪基线 **提升 18–21%**。  
  - 线程上限 `K` 越大、准入控制策略越精细，吞吐越高。

---

## 贡献一句话  
首次**联合 DNN 划分 + 云节点线程并行** 来最大化 **延迟敏感推理请求** 的 **系统级吞吐**，并给出**离线近似 + 在线竞争** 两套可证明算法。



## 3. Deep Reinforcement Learning for Containerized Edge Intelligence Inference Request Processing in IoT Edge Computing  
- **原文链接**: https://ieeexplore.ieee.org/document/10268016   
- **作者**: Jing Li; Weifa Liang; Yuchen Li; Zichuan Xu; Xiaohua Jia; Song Guo  
- **期刊**: IEEE Transactions on Services Computing  
- **标签**: #移动边缘计算（MEC） #DNN 模型推理配置 #吞吐量最大化 #智能物联网设备 #近似和在线算法 #延迟感知 DNN 推理 #DNN 分区 #推理并行 #计算和带宽资源分配与优化 #算法设计和分析   

> ### 摘要 
边缘智能 (EI) 是指一组用于在数据收集点附近收集和学习人工智能 (AI) 数据的互联系统和设备。EI 模型推理阶段已通过边缘缓存技术（例如智能模型 (IM)）得到改进。跨异构分布式边缘节点的 IM 推理值得探讨。本文重点关注软件定义基础设施 (SDI)，并介绍了一种用于移动可穿戴物联网 (IoT) 系统的容器化 EI 框架。该框架称为容器化边缘智能框架 (CEIF)，是一种互通架构，允许配置与移动可穿戴物联网系统相关的容器化 EI 处理智能服务。CEIF 支持对已在云端预训练的 AI 模型的推理服务进行动态实例化。它还支持运行容器虚拟化技术的边缘计算设备 (ECD)。动态 AI 学习策略还可以帮助优化工作负载，从而缩短 EI 推理请求的响应时间。为了在推断收集的数据进行分析时，延缓用户工作负载的快速增长，我们提出了一种深度 Q 学习算法，其中容器集群平台学习每个 ECD 所在位置的用户工作负载变化。EI 推理的请求会根据学习到的值进行缩放，并成功处理，而不会导致 ECD 过载。在案例研究中进行评估时，该算法能够在容器化 EI 系统中扩展 EI 推理的处理请求，同时最大限度地减少实例化的容器 EI 实例数量。EI 推理的请求在负载较低的容器 EI 集群系统中完成。

---

| 现实场景        | 奶茶店的类比                                                                                                |
| ----------- | ----------------------------------------------------------------------------------------------------- |
| **树莓派**     | 街边小店，只能同时做几杯奶茶                                                                                        |
| **AI 推理请求** | 顾客点单：我要一杯“跑步识别奶茶”                                                                                     |
| **容器**      | 每杯奶茶用一个“奶茶机”做                                                                                         |
| **云端存储**    | 总部把配方（AI 模型）提前发到店里                                                                                    |
| **问题**      | 顾客突然暴增，奶茶机不够 → 排队、超时、顾客走人                                                                             |
| **本文做的事**   | 装了一个“**AI 店长**”，实时看排队人数、机器空闲数，**自动决定**：<br>• 再开几台奶茶机（ScaleOut）<br>• 关掉闲置机器（ScaleIn）<br>• 或者不做动作（NONE） |
| **目标**      | 让顾客 **都按时拿到奶茶**，同时 **店里不空转、不超载**                                                                      |


## 研究痛点
- **IoT 可穿戴设备** 持续产生 AI 推理请求（如活动识别）。  
- **边缘节点（RPi）** 资源有限：CPU、内存、并发容器实例数均受限。  
- **传统静态部署** 要么实例过多→资源浪费，要么过少→请求阻塞/超时。  
- **需求**：在保证 QoS（低延迟、高成功率）的前提下，**动态伸缩**容器化 AI 推理实例，**最小化资源占用**。

---

## 核心贡献
| 编号 | 内容 | 亮点 |
|---|---|---|
| 1 | **CEIF 框架** | 基于 Docker-Swarm 的容器化 EI 服务三层架构，支持 MLOps、模型云端同步、实时监控。 |
| 2 | **DRL-Scaling 算法** | 以 **Deep Q-Learning** 为核心，将“何时扩/缩容”建模为 MDP，在线学习最优策略。 |
| 3 | **真实验证** | 5×RPi 集群 + MHEALTH 数据集 + Locust 压测，**吞吐提升 13–32%**，失败率显著下降。 |

---

## 系统架构（CEIF） 
flowchart TD
    Cloud[云端存储] --> Model[预训练模型]
    Model --> DMN[Docker-Swarm Manager\n(DMN)]
    DMN --> Worker1[Worker-1\nRPi + CNT]
    DMN --> Worker2[Worker-2\nRPi + CNT]
    DMN --> WorkerN[… Worker-n\nRPi + CNT]
- **DMN**：Docker-Swarm Manager，负责编排。  
- **Worker**：RPi4 运行容器实例（CNT），每节点一个实例。  
- **监控**：Locust 产生负载 → InfluxDB 记录 → Grafana 实时展示。

---

## DRL 方案设计
### MDP 模型
| 元素 | 定义 |
|---|---|
| **状态 S** | 三元组 `(I, J, K)`：当前容器数、忙碌实例数、排队长度。 |
| **动作 A** | `{ScaleIn, ScaleOut, NONE}`：增/删/不变容器数量。 |
| **奖励 R** | 负增量失败数 + 容器数量惩罚，目标：失败率 & 响应时间 ≤阈值。 |

### 算法流程
1. **离线训练**：利用 RPi 日志数据训练 DQN（主网络+目标网络，经验回放）。  
2. **在线推理**：ε-greedy 选择动作，实时伸缩容器实例。  
3. **状态压缩**：将 EI 请求与资源子空间分解为多个子-MDP，组合训练降低复杂度。

---

## 实验结果
| 场景 | 对比方法 | 关键指标 | 提升 |
|---|---|---|---|
| **30%/50%/80%负载** | MIRAS、DQL-DM | 平均 CPU 利用率、失败请求数、未用容器数 | **CEIF-DRL 优于两者 13–32%** |
| **并发用户 1–12** | 无伸缩策略 | 响应时间从 1 s → 74 s，失败率 87% | **DRL 将失败请求压到接近 0** |

---

## 一句话总结
本文提出 **“容器化边缘智能框架 + DQN 在线伸缩”** 的组合拳，让**树莓派**也能像小型云一样，**按需、低耗、高可靠**地支撑 AI 推理服务。


