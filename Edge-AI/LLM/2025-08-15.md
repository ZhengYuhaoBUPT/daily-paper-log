# 📚 阅读日志 | 2025-08-17  
> 今日目标：10 篇 | 实际完成：1 / 10  

---

## ① EdgeShard: Efficient LLM Inference via Collaborative Edge Computing  
- **原文链接**: [EdgeShard: Efficient LLM Inference via Collaborative Edge Computing](https://ieeexplore.ieee.org/document/9778345)  
- **作者**: Mingjin Zhang, Xiaoming Shen, Jiannong Cao, Zeyang Cui, Shan Jiang  
- **阅读时间**: 60 min  
- **标签**: #边缘计算 #大型语言模型 #协同计算 #模型分区

> **TL;DR**  
> 针对大型语言模型（LLM）在边缘计算环境中的高效部署问题，提出了一种名为 **EdgeShard** 的框架，通过将 LLM 分区并部署在分布式边缘设备和云服务器上，实现了高效的协同推理，显著降低了推理延迟并提高了吞吐量。

---

### 核心痛点  
| 问题 | 原因 | 传统方案缺陷 |
|---|---|---|
| **延迟高** | 依赖云端计算，数据传输延迟高 | 云边缘协作方法依赖高带宽，延迟难以优化 |
| **带宽成本高** | 大量数据传输至云端 | 传统方法未考虑带宽限制，成本高 |
| **隐私问题** | 敏感数据传输至云端 | 云中心处理数据，隐私风险高 |
| **资源受限** | 边缘设备资源有限，难以部署 LLM | 模型压缩方法会导致精度损失，云边缘协作依赖高延迟的云 |

---

### 解决方案 (EdgeShard)  
#### 1️⃣ 框架设计  
- **协同边缘计算 (CEC)**：整合地理分布的边缘设备和云服务器资源，优化资源利用，降低延迟，提高性能。  
- **EdgeShard 框架**：将 LLM 分区为多个碎片（shards），并根据设备的计算和网络资源动态分配到不同的设备上。

#### 2️⃣ 模型分区与设备选择  
- **动态规划算法**：通过动态规划算法优化模型分区和设备选择，最小化推理延迟并最大化吞吐量。  
- **KV 缓存管理**：预分配 KV 缓存空间，避免因缓存覆盖导致的推理错误。

#### 3️⃣ 协同推理  
- **顺序推理**：适用于单用户场景，设备依次处理数据，优化延迟。  
- **流水线并行推理**：通过微批次处理，提高资源利用率，优化吞吐量。

---

### 性能结果  
| 指标 | EdgeShard | 传统方法 (Edge-Solo, Cloud-Edge-Even, Cloud-Edge-Opt) |
|---|---|---|
| **推理延迟** | 最低延迟 75.88ms (Llama2-7B) | EdgeShard 比传统方法快 1.85-3 倍 |
| **吞吐量** | 最高吞吐量 52.45 tokens/s (Llama2-7B) | EdgeShard 比传统方法高 2.2-7 倍 |
| **内存优化** | 通过分区，显著降低单设备内存需求 | 传统方法因内存限制无法部署大模型 |

---

在《EdgeShard: Efficient LLM Inference via Collaborative Edge Computing》这篇文章中，对比基线算法如下：

### Edge-Solo
- **出处**：该算法是基于边缘计算的本地部署方法，没有特定的文献出处，是边缘计算场景下的一种常见部署方式。
- **提出时间**：不明确，但随着边缘计算的发展而逐渐出现。

### Cloud-Edge-Even
- **出处**：该算法是将 LLM 均匀划分为两部分，分别部署在边缘设备和云服务器上，是一种简单的云边协作方法。
- **提出时间**：不明确，但这种均匀划分的思路在云边协作的研究中较为常见，可能较早出现。

### Cloud-Edge-Opt
- **出处**：该算法在文章中提到使用了所提出的动态规划算法进行 LLM 分区，与 EdgeShard 的区别在于只有两个设备作为算法输入。因此，其核心思想与 EdgeShard 有一定的联系，但具体实现和应用场景有所不同。
- **提出时间**：不明确，但可以推测是在 EdgeShard 研究过程中或之前的相关研究中提出的。

### 一句话总结  
EdgeShard 通过协同边缘计算和动态规划算法，实现了 LLM 在边缘设备和云服务器上的高效分布式推理，显著降低了推理延迟并提高了吞吐量，同时解决了边缘设备资源受限和隐私问题。


## ② Enhancing LLM QoS Through Cloud-Edge Collaboration: A Diffusion-Based Multi-Agent Reinforcement Learning Approach  
- **原文链接**: https://ieeexplore.ieee.org/document/10970093   
- **作者**: Zhi Yao, Zhiqing Tang, Wenmian Yang, Weijia Jia  
- **阅读时间**: 60 min  
- **标签**: #边缘计算 #向量数据库 #扩散模型 #多智能体强化学习 #请求调度

> **TL;DR**  
> 针对大型语言模型（LLM）在云端部署导致的响应延迟高和成本高的问题，提出了一种基于向量数据库辅助的云边协同优化框架（VELO），通过在边缘服务器上缓存 LLM 请求结果，显著降低了后续类似请求的响应时间。该框架利用扩散模型和多智能体强化学习（MARL）算法，动态决定是否从云端请求 LLM 或从边缘向量数据库中检索结果，显著提升了边缘用户的满意度。

---

### 核心痛点  
| 问题 | 原因 | 传统方案缺陷 |
|---|---|---|
| **响应延迟高** | LLM 依赖云端计算，数据传输延迟高 | 缓存技术未充分利用，响应时间长 |
| **成本高** | 云端计算资源消耗大 | 缓存策略不完善，资源浪费 |
| **隐私问题** | 敏感数据传输至云端 | 缺乏有效的隐私保护机制 |

---

### 解决方案 (VELO)  
#### 1️⃣ 框架设计  
- **向量数据库缓存**：在边缘服务器上部署向量数据库，缓存 LLM 请求结果，减少后续类似请求的响应时间。
- **动态请求调度**：通过扩散模型和多智能体强化学习算法，动态决定请求的处理位置，优化 QoS。

#### 2️⃣ 算法设计  
| 算法 | 作用 | 关键思想 |
|---|---|---|
| **DLRS (Diffusion-based LLM Request Scheduling)** | 动态请求调度 | 利用扩散模型提取请求特征，结合向量查询结果，决定请求处理位置 |
| **特征提取网络** | 提取请求特征 | 使用 Transformer 编码器提取请求特征，结合向量查询结果，优化决策 |
| **专家演示训练** | 提高训练效率 | 在训练过程中引入专家演示，解决数据稀疏和早期探索问题 |

---

### 性能结果  
| 指标 | DLRS vs 基线方法 | 改进百分比 |
|---|---|---|
| **满意度** | 平均提升 15.0% | - |
| **延迟** | 平均降低 14.6% | - |
| **资源消耗** | 显著降低 | - |

---

### 一句话总结  
通过向量数据库缓存和基于扩散模型的多智能体强化学习算法，VELO 框架显著提升了 LLM 在边缘计算环境中的响应速度和资源利用效率，同时保护了用户隐私。

---
### 实验设置
系统实现：使用 Python 实现 VELO 原型系统，部署在边缘服务器和云服务器上。边缘服务器包括 Towhee 服务和 Milvus 服务，均通过容器部署。实验平台由三台桌面计算机组成，每台配备 Intel i9-10900K 10 核 CPU 和 NVIDIA RTX 2070Super GPU。数据通过 TCP 协议传输到训练服务器，通过 FTP 协议获取最新网络权重。
数据集：使用多语言开放对话数据集 oasst1 作为 LLM 请求数据。训练集由 oasst1 中最高排名的对话扩展为五种语言版本（英语、西班牙语、德语、中文和俄语）。测试集通过 Qwen14b 重述 QA 对中的问题获得。
参数设置：训练数据集大小为 3000 个样本，训练轮数为 13500 轮，测试轮数为 500 轮。策略网络和价值网络的学习率分别设置为 0.0003 和 0.001，折扣因子设置为 0.99。DLRS 算法调整噪声强度，需要 10 个去噪步骤来恢复行动决策。
基线方法：包括 Greedy-0.1、Greedy-0.3、Greedy-0.5、Greedy-LLM、MAPPO、G-MAPPO、T-MAPPO、LRS、DLRS-L5、DLRS-L15、DLRS-V5、DLRS-V10、DLRS-V15 和 Random 算法。
评估方法：将 LLM 请求卸载到最近的服务器，以及将 LLM 请求卸载到所有服务器，并返回最快响应作为请求的答案。

---

## ③ Beyond the Cloud: Edge Inference for Generative Large Language Models in Wireless Networks  
- **原文链接**: https://ieeexplore.ieee.org/document/10759588   
- **作者**: Xinyuan Zhang, Jiangtian Nie, Yudong Huang, Gaochang Xie, Zehui Xiong, Jiang Liu, Dusit Niyato, Xuemin Shen  
- **阅读时间**: 60 min  
- **标签**: #无线网络 #边缘计算 #大型语言模型 #生成式AI #模型量化

> **TL;DR**  
> 针对大型语言模型（LLM）在云端部署导致的响应延迟高、成本高和隐私问题，提出了一种基于无线边缘网络的 LLM 边缘推理框架，通过动态批处理和模型量化技术，显著提高了资源受限的边缘设备上的推理吞吐量，同时满足用户对延迟和准确性的要求。

---

### 核心痛点  
| 问题 | 原因 | 传统方案缺陷 |
|---|---|---|
| **响应延迟高** | LLM 依赖云端计算，数据传输延迟高 | 缓存技术未充分利用，响应时间长 |
| **成本高** | 云端计算资源消耗大 | 缓存策略不完善，资源浪费 |
| **隐私问题** | 敏感数据传输至云端 | 缺乏有效的隐私保护机制 |

---

### 解决方案 (VELO)  
#### 1️⃣ 框架设计  
- **动态批处理**：通过动态调整批处理大小，优化边缘设备的资源利用，提高推理吞吐量。
- **模型量化**：采用模型量化技术，减少模型在边缘设备上的内存占用和计算延迟。

#### 2️⃣ 算法设计  
| 算法 | 作用 | 关键思想 |
|---|---|---|
| **OT-GAH (Optimal Tree-search with Generalized Assignment Heuristics)** | 动态请求调度 | 利用树搜索和在线剪枝技术，优化请求调度，提高吞吐量 |
| **特征提取网络** | 提取请求特征 | 使用 Transformer 编码器提取请求特征，结合向量查询结果，优化决策 |

---

### 实验设置  
- **系统实现**：使用 Python 实现 LLM 边缘推理框架，部署在无线边缘网络中。实验平台包括边缘服务器、无人机和车辆，每台设备配备 NVIDIA JETSON TX2 GPU。
- **数据集**：用户请求到达率从 5 到 250 requests/second，输入提示长度和生成序列长度随机选择。
- **参数设置**：边缘服务器配备 20 个 NVIDIA JETSON TX2 GPU，每个 GPU 1.33 TFLOPs 和 32GB 内存。无人机和车辆分别配备 1 个和 3 个 GPU。
- **基线方法**：包括静态批处理和无批处理两种方法。

---

### 性能结果  
| 指标 | OT-GAH vs 基线方法 | 改进百分比 |
|---|---|---|
| **吞吐量** | 平均提升 45% | - |
| **延迟** | 平均降低 20% | - |
| **资源消耗** | 显著降低 | - |

---

### 一句话总结  
通过动态批处理和模型量化技术，提出的 LLM 边缘推理框架显著提高了无线边缘网络中的推理吞吐量，同时满足了用户对延迟和准确性的要求，保护了用户隐私。
