# 📡 论文速览：基于 GRU-SAC 的卫星边缘计算卸载-缓存联合优化

&gt; 题目：Joint Optimization of Computation Offloading and Caching for Satellite Edge Computing Based on GRU-SAC Algorithm  
&gt; 作者：Lu Dong 等  
&gt; 会议/期刊：IEEE Internet of Things Journal（2025 early access）

---

## 1️⃣ Introduction | 研究背景与动机
- 地面 MEC 难以覆盖海洋、山区、灾区，LEO 卫星边缘计算（SEC）成为补充。
- 卫星高速移动 → 覆盖时长短、拓扑高动态；星上资源异构 → 卸载决策复杂。
- 任务所需数据库高度相似 → 冗余传输严重，亟需“星上缓存”。
- **核心挑战**：如何在“高动态+异构资源+缓存空间受限”下，联合优化卸载与缓存，使**时延**与**能耗**双降。

---

## 2️⃣ Related Work | 文献对比
| 文献 | 是否考虑缓存 | 是否预测请求 | 是否联合优化 | 方法 | 局限性 |
|---|---|---|---|---|---|
| [19]-[30] | ❌/简单 LRU | ❌ | ❌/部分 | DDPG、PPO、D3QN 等 | 忽略数据重复性、星间协同、动态请求模式 |
| **本文** | ✅ 星间协同缓存 | ✅ GRU 预测 | ✅ 卸载+缓存联合 | **GRU-SAC** | 填补上述空白 |

---

## 3️⃣ System Model | 模型与公式
- **3.1 网络模型**：用户-卫星-云三层；卫星覆盖时间几何建模。
- **3.2 缓存模型**：星上容量限制，缓存决策 Δcᵢₘ(t)∈{-1,0,1}。
- **3.3 通信模型**：上下行 Shannon 容量，自由空间损耗。
- **3.4 计算模型**：
  - 本地执行 / 星上执行 / 云上执行 三类时延与能耗闭合式。
- **3.5 问题形式化**：  
min Σ α·Tᵢᵗ + (1-α)·Eᵢᵗ
s.t. 剩余覆盖时间、缓存容量、状态合法更新

→ NP-完全，传统启发式难以在线求解。

---

## 4️⃣ GRU-SAC Algorithm | 方法设计
| 模块 | 功能 | 关键技术 |
|---|---|---|
| **GRU 预测** | 输出未来请求概率 Nᵗᴵ | 请求计数、比例、近期性三特征 |
| **MDP 状态** | sᵗ = {卫星资源、覆盖、缓存、预测概率} | 高维连续空间 |
| **动作** | 同时决定：①卸载位置 bᵢ ②缓存更新 Δcᵢₘ | 混合离散-连续 |
| **SAC 核心** | 最大熵强化学习，双 Q 网络抑制过估计 | 目标软更新，经验回放 |
| **训练流程** | Algorithm 1 | 预测模型与决策模型交替迭代 |

---

## 5️⃣ Simulation | 实验结果
- **5.1 场景**：5 用户 + 2 卫星 + 1 云；参数见 Table III。
- **5.2 Baseline**：
1. NC-SAC：无缓存纯卸载
2. LRU-SAC：传统 LRU 缓存 + SAC 卸载
- **5.3 关键指标**：奖励值（负的加权时延+能耗）、平均时延、平均能耗。

| 实验组 | 主要结论 |
|---|---|
| **权重敏感性** | 任意 α∈[0.3,0.7] 下，GRU-SAC 奖励最高，时延/能耗均最低 |
| **缓存容量缩放** | 缓存从 1→3 任务，GRU-SAC 奖励↑12%，时延↓3%，能耗↓11% |
| **请求概率突变** | 160 回合后概率翻转，MAE≈0.02 几乎不变，奖励快速收敛 → 鲁棒 |
| **扩展性** | 8 用户/3 卫星场景，优势依旧；作者计划引入注意力机制进一步放大规模 |

---

## 6️⃣ Conclusion | 结论与展望
- 提出**首个**面向 SEC 的“请求预测-卸载-缓存”联合框架。
- GRU-SAC 在多项指标上**稳定优于**无缓存/LRU 缓存方案：
- 时延降低 ≈ 2%
- 能耗降低 ≥ 11%
- 未来工作：
1. 引入**局部-全局注意力**机制，支持千级卫星扩展。
2. 考虑**星间链路**、**多任务类型**、**安全隐私**。
3. 真实卫星 traces 在线验证。

---

## 🖼️ 一句话部署图
Copy
地面云服务中心（Agent 驻地）
├─ GRU 预测模块  ——→ 输入：历史请求日志
├─ SAC-Actor      ——→ 输出：b_i , Δc_{im}
├─ SAC-Critic×2   ——→ 价值评估
└─ 馈电链路
     ↑全局状态         ↓决策命令
卫星星座 & 地面用户

---

## ⚠️ 注意
卫星上只跑“执行脚本”（接收缓存/卸载指令，本地调度 CPU 与存储），不跑训练。
用户终端只跑“轻量客户端”（上报任务请求、接收卸载目的地），不跑推理。
这样部署才能满足：
全局状态可观测（云中心天然拥有全网信令）；
持续算力可训练（云 GPU 池）；
卫星/终端资源不超载（只做执行）。

---

## 🌟 创新点（Innovations）
1. **问题新**：首次将“可重用数据缓存”纳入 SEC 卸载优化，并考虑卫星覆盖剩余时间、星间资源异构。
2. **方法新**：
 - 提出**GRU-SAC 双层架构**：GRU 预测请求概率 → SAC 输出卸载+缓存联合动作。
 - 最大熵强化学习保证**探索-利用平衡**，避免局部最优。
3. **性能新**：在动态请求、缓存容量突变等场景下**鲁棒收敛**，多项指标**全面碾压**现有 DRL 方法。

---
> 一键引用（BibTeX）：
```bibtex
@article{gru-sac-sec2025,
title={Joint Optimization of Computation Offloading and Caching for Satellite Edge Computing Based on GRU-SAC Algorithm},
author={Dong, Lu and Xu, Aoting and Li, Yueqi and Liu, Jian and Jia, Yubin},
journal={IEEE Internet of Things Journal},
year={2025},
doi={10.1109/JIOT.2025.3598758}
}
```



# 🛰️ 论文速览：Multi-Layer RL Assisted Task Offloading in Satellite Edge Computing  
&gt; 题目：Multi-Layer Reinforcement Learning Assisted Task Offloading in Satellite Edge Computing  
&gt; 作者：Weichang Wen, Haixia Cui, Tao He  
&gt; 期刊：IEEE TVT, 2025年4月, Vol.74, No.4

---

## 1️⃣ Introduction | 研究背景与痛点
- **卫星=全球无缝覆盖**：沙漠、远洋、灾区等地面网络盲区唯一选择。
- **SEC 瓶颈**：星上计算/储能有限、拓扑高动态、链路频繁中断 → 任务易失败、时延高、能耗大。
- **现有工作缺陷**：
  - 单层/双层星座结构简单，无法兼顾“超低时延”与“海量算力”；
  - 集中式调度开销大、单点失效风险高；
  - 传统 RL 仅依赖本地观测，初期决策偏差大。
- **作者目标**：设计“三层星座 + 分层 RL”框架，实现**低时延、低能耗、高成功率**的分布式任务卸载。

---

## 2️⃣ System Model | 三层星座与成本建模
| 层级 | 轨道 | 角色 | 算力 | 时延 | 备注 |
|---|---|---|---|---|---|
| **Cloud** | GEO | 云中心 | 最强 | 最高 | 全局状态、集中训练 |
| **Edge** | MEO | 边缘数据中心 | 中等 | 中等 | 区域汇聚 |
| **Mist** | LEO | 雾节点 | 最低 | 最低 | 本地卸载首选 |

- **链路模型**：三维坐标 + Heron 公式计算地球遮挡；自由空间/多径能耗分段。
- **本地/星上/云处理**时延与能耗闭合式已给出。
- **优化目标**：  
min  φ1·时延 + φ2·能耗
s.t. 单选卸载目标、最大容忍时延、星上 CPU 上限


---

## 3️⃣ Multi-Layer RL Solution | 算法设计
### 3A 基础 Q-Learning
- 每个节点维护**本地 Q-表**，状态=〈任务量、CPU 占用〉，动作={本地, LEO, MEO, GEO}。
- ε-Greedy 探索，奖励=按时完成则“负时延-能耗”，超时则惩罚。

### 3B 多层代理扩展 (MLRL) 创新点
| 机制 | 说明 | 好处 |
|---|---|---|
| **分层决策** | LEO 先决策；本地知识不足→**向上查询** MEO/GEO | 避免初期盲目决策 |
| **查询奖励修正** | 上级返回的奖励×ω（>1），查询动作 Q 值×φ（<1） | 鼓励“问对人”，同时积累本地经验 |
| **集中训练-分布执行** | GEO 汇总全局状态离线训练；各星在线只查表/查询 | 降低星上计算开销 |

- **伪代码**：Algorithm 2 给出完整交互流程。

---

## 4️⃣ Simulation | 实验验证
- **平台**：SatEdgeSim + STK 轨道数据；设备 100–1000 台；任务三类（AR/复杂 App/数据处理）。
- **Baseline**：
1. Trade-off 均衡调度
2. 权重贪婪算法
3. Random-VM
- **指标**：平均端到端时延、平均能耗、任务成功率、失败原因、各层任务分布。

### 🔑 关键结果
| 指标 | MLRL 表现 | 相对提升 |
|---|---|---|
| **端到端时延** | 最低（图5） | ↓ 10–15% vs RL，↓ 25% vs 贪婪 |
| **能耗** | 略增但可接受（图6） | ↓ 5–7% vs 贪婪 |
| **成功率** | >90%（图7） | ↑ 8–12% vs 其余算法 |
| **失败任务数** | 延迟/移动性失败最少（图8） | ↓ 30%+ vs 本地 RL |
| **任务分布** | 更多任务被推向**边缘 & 雾**（图9） | 符合“低时延优先近端”原则 |

---

## 5️⃣ Conclusion | 结论与展望
- 提出**三层星座 + MLRL**卸载框架，首次把“**向上查询-知识蒸馏**”引入卫星 RL；
- 集中训练保证全局最优，分布执行+查询机制解决**本地观测不足**痛点；
- 在时延、成功率上**稳定优于**传统贪婪、单级 RL 与随机策略；
- 未来方向：
1. 引入**深度 RL**（DQN/PPO）应对连续状态/动作；
2. 考虑**星间链路切换**、**任务依赖图**；
3. 真实卫星集群在轨试验。

---

## 只有当“本地 Q-table 对当前状态-动作的价值信心不足”时，LEO 雾节点才会主动向上级（MEO/GEO）发起查询请求。
### 触发条件（论文原文映射）
Table
Copy
条件	具体表现	出处
1. 本地 Q 值过低	对四个可选动作（本地/LEO/MEO/GEO）的最大 Q(s,a) < 预设阈值 θ	Algorithm 2 第 6-10 行
2. ε-Greedy 选中“查询动作”	动作空间被作者显式加入一个虚拟动作 a<sub>；当 ε-随机过程选中它即触发查询	Algorithm 2 第 11 行注释
3. 经验惩罚累积	过去 k 次决策中采用“本地决策”的失败率 > φ，则强制下一次走查询	正文 III-C 第三段 “query penalty factor φ”

---

## Agent 的“大脑”放在地面云服务中心（GEO 云节点），“反射神经”下沉到各 LEO/MEO 卫星节点——形成“地面集中训练 + 星上分布执行”的两级部署。
### 物理映射表
| 层级           | 实体          | 是否驻留 Agent 代码                  | 作用                          | 原文依据                            |
| ------------ | ----------- | ------------------------------ | --------------------------- | ------------------------------- |
| **GEO 云中心**  | 高性能地面站/高轨卫星 | ✅ 完整 Agent（含全局 Q-table、训练线程）   | 集中训练、汇总全星座状态、下发策略           | III-C 末段 “centralized training” |
| **MEO 边缘节点** | 中轨卫星        | ✅ 轻量 Agent（只读全局表 + 本地查询接口）     | 区域决策、接受 LEO 查询、向 GEO 回传体验数据 | Fig.4 “Upper Layer Agent”       |
| **LEO 雾节点**  | 低轨卫星        | ✅ 最轻 Agent（本地 Q-table + 查询客户端） | 实时卸载决策、信心不足时向上查询            | Fig.4 “Mist Agent”              |
| **地面终端**     | IoT/车载/船载   | ❌ 纯客户端                         | 上报任务、接收卸载指令                 | Table IV “Generate tasks: YES”  |

### 数据面链路
终端   
  │ 任务请求   
  ▼   
LEO（本地 Q-table 决策）   
  ├─信心足 → 直接执行  
  └─信心不足 → 查询包 ▶ MEO/GEO  
          ▲  动作&奖励  ◀──┘  
### 训练面链路
GEO 收集全网体验 → 离线更新全局 Q-table → 定期向 LEO/MEO 推送增量参数

---

## 🌟 创新点（Highlights）
1. **架构新**：GEO-MEO-LEO 三层协同星座，兼顾“超低时延”与“海量算力”。
2. **算法新**：
 - **MLRL** = 传统 Q-Learning + 分层查询 + 奖励修正；
 - **集中训练-分布执行**降低星上开销。
3. **机制新**：本地知识不足→**向上级代理查询**，避免初期错误决策，兼顾探索与利用。
4. **验证全**：SatEdgeSim+STK 大规模仿真，**时延、能耗、成功率、失败原因**多指标全面领先。

---
> BibTeX 一键引用
```bibtex
@article{wen2025mlrl,
title={Multi-Layer Reinforcement Learning Assisted Task Offloading in Satellite Edge Computing},
author={Wen, Weichang and Cui, Haixia and He, Tao},
journal={IEEE Trans. Veh. Technol.},
volume={74},
number={4},
pages={6561--6572},
year={2025}
}
```