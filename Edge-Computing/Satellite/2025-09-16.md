# 📡 文章全景解读 | SusCO: 让“商业锅”成为 LEO 卫星的梯子  
&gt; 会议：IEEE INFOCOM 2025  
&gt; 题目：Commercial Dishes Can Be My Ladder: Sustainable and Collaborative Data Offloading in LEO Satellite Networks  
&gt; 作者：Yi Ching Chou 等  
&gt; 关键词：LEO 星座、商业地面站、反向拍卖、协作卸载、可持续性

---

## 1️⃣ 引言（Introduction）——为什么“只用星”或“只用地”都不够？

| 维度 | Bent-Pipe 直传 | ISL 星间路由 | 商业锅（SusCO） |
|---|---|---|---|
| 地面站需求 | 极密（&gt;100×GEO） | 0 | 0（复用现有） |
| 单星电池 | 低负载 | 高负载→寿命↓ | 提前卸载→寿命↑ |
| 资本支出 | 新建站 $$$ | 0 | 按需租用 $ |
| QoS 弹性 | 固定链路 | 排队/拥塞 | 多锅聚合带宽 |

- 作者提出**SusCO**框架：卫星在“电池告急”前，把数据通过反向拍卖卸载给**商业地面站+5G基站**（统称 commercial dishes），再走地面网到达目的地。
- 核心问题：
  1. 如何量化“卸载”对**能耗、时延、寿命**的收益？
  2. 如何从众多**QoS/价格各异**的 GSaaS 提供商中**组队**？
  3. 如何设计**激励相容**的支付机制，让锅主愿意接单？

---

## 2️⃣ 背景与动机（Background & Motivation）

### 2.1 Bent-Pipe 的“土豪”困境
- 550 km LEO 单星仅覆盖 **1.7%** 地球表面积；全球无缝需 **&gt;10×MEO/GEO** 的地面站数量（图 2）。
- 新建站成本 = 土地 + 许可 + 设备 + 运维，**初创星座难以承受**。

### 2.2 ISL 的“电池”困境
- 每多一跳星间链路 = 一次**接收-存储-转发**，电池持续放电。
- LEO 每 90 min 进入地球阴影**2次**，电池深度循环频繁（图 3）。
- **低电量时能耗对寿命伤害呈指数级放大**（文献[11]模型）。

### 2.3 GSaaS 的“甜蜜”与“陷阱”
- AWS Ground Station、Azure Orbital 等已上线，**按分钟计费**。
- 陷阱：单锅 QoS 差异大、**单点可能容量不足**；若**多锅简单并联**，又面临**组合爆炸+预算失控**。

→ 需要**系统化、可协作、带激励**的多锅卸载框架，这就是 SusCO 的出发点。

---

## 3️⃣ SusCO 框架与模型（Framework & Models）

### 3.1 网络场景
- 三层拓扑：
  - 星间网（ISL）
  - 星地网（GSL）
  - 地面网（Internet/5G）
- 虚拟拍卖平台**内嵌在 LEO 卫星**（容器/轻量 VM），每 60 s 一个拍卖区间 τ。

### 3.2 反向拍卖流程（图 5）
1. **Task 发布**：卫星 vi 把待卸载任务 tτi,j 广播给覆盖范围内的锅。
2. **Bid 提交**：锅 vk 返回五元组  
   bk(τ) = ⟨ξk, γk, Δk, ck, vi′⟩  
   分别表示：到目的地时延、带宽、可接收数据量、成本、建议卸载星（可≠vi）。
3. **候选组生成**（算法 1）：卫星侧运行 CGSC，生成**满足 QoS & 预算**的协作组集合 Gi,j(τ)。
4. **赢家选择与支付**（算法 2）：卫星侧运行 CSTP，按**效用/成本比**选组，并用**第二价格思路**确定每口锅的支付额。
5. **数据卸载 & 付款**：任务流量被**拆分**到组内各锅，成功后按实际接收量付费。

### 3.3 效用模型（三合一）
总效用 Ui,j→λτ 由三部分加权：
- **能耗降低** UE：卸载后剩余星跳节省的能耗（归一化）。
- **时延改善** UD：地面路径 vs 纯星路径的 E2E 延迟差。
- **寿命延长** UL：基于电池模型[11]的“寿命成本”减少量（低电量时权重指数级上升）。

权重可调（默认 w1=0.3, w2=0.4, w3=0.3），运营商可在 Dashboard 拖滑块。

---

## 4️⃣ 问题形式化与算法设计（Problem & Algorithms）

### 4.1 CDGS 优化问题
最大化总效用  
s.t.
- 每个任务最多选一个协作组（xτi,j→λ ∈ {0,1}）
- 时延、带宽、数据量 QoS
- 总支付 ≤ 任务预算 βτ
- 单锅支付 ≥ 其成本 ck（个体理性）

→ **NP-hard 组合拍卖**，穷举 2^|B| 不可行。

### 4.2 CGSC：候选组构造（算法 1）
- **延迟预过滤**：单锅时延 &gt; 任务要求则剔除。
- **分层合并**：  
  层-1 = 单锅组；层-n 由层-(n-1) 中 Top-M 最小成本组两两合并，直到层-N。
- **容量 & 预算二次剪枝**，保证可扩展。
- 复杂度 ≈ O(N·M·|B|)，实测 N=2, M=10 时在 1584 颗 Starlink 规模下 **5.5 ms** 完成。

### 4.3 CSTP：赢家选择与支付（算法 2）
- 对每组计算 **效用/成本比** + **探索因子**（与被选次数 nλ 成反比）。
- 选比值最高者中标，支付额 =  
  pτi,j→λ = (第二高比值 × 总成本) + 微调项，**类似 VCG 思想**。
- 单锅最终支付按**成本占比**拆分，确保 **ck ≤ pk**（个体理性）且 **预算可行**。
- 线性扫描，实测 69 ms（Starlink 场景）。

---

## 5️⃣ 性能评估（Performance Evaluation）

### 5.1 仿真设置
- 真实星座参数：Starlink/Telesat/OneWeb/Kuiper，卫星数 72→3168。
- 商业锅分布：AWS Ground Station 公开坐标 + 5G NR 基站按人口密度生成。
- 业务流：300 Mb/s 聚合，300 s 仿真，100 个拍卖区间。
- 对比基准：
  1. SERvICE：仅考虑时延+带宽，单锅单路径。
  2. SMTSN：考虑寿命+时延，但**无协作组**。
  3. FALCON：选最小延迟单路径，**无视预算与寿命**。

### 5.2 核心结果（图 7 & 表 1）
| 指标 | SusCO 提升（相对次优） |
|---|---|
| **卫星寿命消耗减少** | 26.75 % |
| **能耗减少** | 26.07 % |
| **端到端延迟减少** | 32.77 % |
| **效用/成本比** | 最高，预算利用率 ↑ 25 % |

- **CDF 曲线**：在严格延迟 &lt; 120 ms 区间内，SusCO 任务完成率比 FALCON 高 18 %。
- **可扩展性**：算法总运行时在 2×Starlink（3168 星）场景下 **95 ms**，呈**线性**增长，适合星上实时部署。

---

## 6️⃣ 相关研究（Related Work）
- **纯 bent-pipe**：Starlink 实测仍为主力，但需要“土豪式”建站（Ma et al. INFOCOM’23）。
- **纯 ISL**：Yang’16、Chou’22 等用电池水平做路由，**未考虑地面卸载**。
- **GSaaS 单路径**：Lyu’23 FALCON 仅选最低延迟路径，**无多锅协作、无激励**。
→ SusCO 首次把“**多锅协作 + 反向拍卖 + 电池寿命模型**”整合进 LEO 路由，并给出**可证明的个体理性与预算可行**。

---

## 7️⃣ 未来工作（Future Work）
1. **信誉系统**：自动记录锅主履约率，引入**信用分+保证金**，防止“跑单”。
2. **动态 SLA**：支持**弹性带宽/突发数据**，价格随实时负载波动。
3. **安全与隐私**：地面段引入**TLS-1.3 加密 & 零知识证明**，防止流量分析。
4. **星上轻量化**：把 CGSC/CSTP 改写为 **Rust+ eBPF**，内存占用 &lt; 64 MB，适配星载容器。

---

## 8️⃣ 结论（Conclusion）
&gt; SusCO 用“商业锅”给 LEO 卫星搭了一把**可持续、可协作、可激励**的梯子：  
&gt; - 卫星提前卸载，**电池寿命↑26 %**  
&gt; - 多锅并行接收，**延迟↓33 %**  
&gt; - 反向拍卖+第二价格，**预算利用率↑25 %**  
&gt; 整套算法**&lt;100 ms** 级运行，已在 3000+ 卫星场景验证，为 6G 时代的“空-地一体流量卸载”提供了**即插即用**的经济模型与工程方案。


---

# 📊 QLLMS 全景解读  
&gt; 论文：QLLMS: Quantization-adaptive LLM Scheduling for Partially Informed Edge Serving Systems  
&gt; 会议：IEEE INFOCOM 2025  
&gt; 学校：中山大学  
&gt; 关键词：大模型量化、边缘 serving、部分信息、稳定匹配、GPU 租赁成本

---

## 1️⃣ 引言（Introduction）——边缘跑大模型的“三座大山”

| 挑战 | 现状 & 痛点 |
|---|---|
| **资源稀缺** | 边缘 GPU 数量少、型号杂，显存/算力远小于云。 |
| **量化带来的不确定性** | 低比特≈低延迟+小内存，但 perplexity 暴涨；如何选“最合适”的比特？ |
| **信息不完整** | 真实部署时，不可能穷举所有{模型，量化，GPU} 三元组做 profiling；大量性能格子是“空”的。 |

- 已有工作把“量化”与“调度”当成两个独立阶段，先固定量化再排任务，导致**次优解**。
- 本文提出 **QLLMS**（Quantization-adaptive LLM Scheduling）：
  1. 联合决定“用几比特”+“放哪张卡”；
  2. 只拿到**部分 profiling 数据**也能工作；
  3. 目标：**GPU 租赁成本最小**，同时满足 perplexity & 延迟 SLO。

---

## 2️⃣ 相关研究（Related Work）——两条主线 & 四个缺口

### ① 大模型边缘化
- **压缩手段**：量化、剪枝、蒸馏、MoE-offload。
- **痛点**：超低位（int3/int4）perplexity 雪崩；尚无“如何选比特”的系统性方法。

### ② 边缘调度
- **经典算法**：EDF、MEF、Lyra、CEFL——**均不考虑量化**。
- **云原生方案**：Mélange 只解决“云-GPU 异构”，**默认全精度**，且需**完整信息**。

→ 四个缺口：
1. 量化与调度**割裂**；
2. 边缘**部分信息**场景；
3. 需同时兼顾**perplexity+延迟**双 SLO；
4. 缺少**理论保证**（稳定性、激励相容）。

---

## 3️⃣ 测量研究（Measurement Study）——用数据说话

### 3.1 实验设计
- **模型**：Llama2-7B/13B、Mistral-7B
- **数据集**：WikiText-2、C4
- **GPU**：A100、V100、RTX3090/4090、A6000（价格 0.2 $/h → 0.85 $/h）
- **指标**：模型大小、内存、perplexity、TTFT、TPOT、token/$

### 3.2 关键发现

| 发现 | 具体数值 | 启示 |
|---|---|---|
| **perplexity ∝ 1/比特数** | int3 比 fp16 高 2.3× | 不能一味追求低比特 |
| **延迟 ∝ 1/比特数** | int4 在 exllama 核下反而比 int8 快 | 需**实测**而非理论外推 |
| **T/$ 峰值** | RTX3090+int4 ≈ A100+fp16 | 低端卡+量化可**平替**高端卡 |
| **GPU  hetero 影响** | 同模型同比特，不同卡 T/$ 差 1.4× | 必须把“卡型号”写进决策空间 |

→ 引出核心思想：**量化、模型、GPU 三维联合 profiling** 是必要且可行的。

---

## 4️⃣ 系统模型与问题建模（System Model & Formulation）

### 4.1 实体
- **任务**：M 个 LLM 推理任务，已知⟨输入长度, 输出长度, 模型族, SLO⟩。
- **资源**：N 类边缘服务器，每类 j 有 Bj 张 GPU，单价 cj $/h。
- **量化**：Q 个离散等级 {fp16, int8, int4, int3}。

### 4.2 决策变量
δi,j,q ∈ {0,1} —— 任务 i 是否被放到服务器 j 并使用量化等级 q。

### 4.3 目标与约束
min ∑δi,j,q · cj  
s.t.
1. 每个任务**唯一**映射；
2. 内存不超；
3. 延迟 ≤ slat_i,th；
4. perplexity ≤ sppl_i,th；
5. 只能选**可用量化集合** Qava_i,j —— 这里引出“部分信息”痛点：Qava_i,j 矩阵**大量缺失**。

→ 把缺失的 Qava_i,j 补全，再求解匹配，就是 QLLMS 的核心使命。

---

## 5️⃣ QLLMS 框架（Framework）——三大模块协同
任务流 → AQS Profiler → AQS Reconstructer → Stable Matching Scheduler → 映射结果


### 5.1 AQS Profiler
- 对**已有**格子做微基准，生成三维矩阵  
  P_M×Q（perplexity）、L_M×N×Q（延迟）。
- 输出：Qava_i,j = {q | p_i,q ≤ sppl_th & l_i,j,q ≤ slat_th}。

### 5.2 AQS Reconstructer——低秩补全
- 观测率 ρ 仅 20 %–50 %，采用**核范数最小化**松弛：
  min ||T||_*  s.t.  S = B ∘ T
- 使用**奇异值阈值（SVT）**算法，复杂度 O(mn log(1/ε))，**10 次迭代内收敛**。
- **定理 1**（Candès et al.）：若真实矩阵秩 r=O(1)，采样数 k ≥ C·l^5/4·r·log l，则**完美恢复**概率 ≥ 1−cl⁻³。
→ 为边缘“只测了 30 % 格子”提供**理论底气**。

### 5.3 Stable Matching Scheduler——多对一延迟接受（DAA）
- **任务侧偏好**：按“**租金单价**”升序排列服务器；量化选**最小可用比特**（省钱）。
- **服务器侧偏好**：按“**SLO 满足度**”升序排列任务（越早满足越好）。
- **QLLMS-DAA** 流程：
  1. 任务向最爱服务器求婚；
  2. 服务器保留**容量内**最爱任务，其余拒绝；
  3. 重复直至无未婚任务。
- **定理 2 & 3**：输出匹配**无阻塞对** → **稳定匹配**；算法复杂度 O(M·N·Q)。

---

## 6️⃣ 实验评估（Performance Evaluation）

### 6.1 设置
- 真实代码：AutoGPTQ + HuggingFace + PyTorch
- 任务流：ChatBot-Arena 100 万对话，TTFT 阈值 1 s，TPOT 50 ms， perplexity < 10。
- 硬件：4 类 GPU（A100/4090/3090/A6000），每类 2 张。
- 对比基线：
  - Random / EDF / MEF / Mélange（云原生，无量化，完整信息）

### 6.2 结果一览

| 场景 | 指标 | QLLMS 提升（vs 次优） |
|---|---|---|
| **完整信息** | 租赁成本 | ↓ 26.3 %（vs EDF） |
|  | 任务边缘完成率 (TCR) | ↑ 12.4 % |
| **50 % 信息缺失** | 租赁成本 | ↓ 22.4 %（vs EDF） |
|  | TCR | ↑ 58.8 % |
| **GPU  hetero** | 成本降幅 | 4.6 %–20.4 % |
| **RTX3090-only** | 成本 | 比 Mélange ↓ 71 %，**全部边缘完成** |

→ 说明：
1. 低端卡+int4 可**平替**高端卡，QLLMS 自动发现；
2. 信息缺失场景，**SVT 补全后仍接近最优**；
3. Mélange 因“无量化”选项，导致大量任务被踢到云，成本暴涨。

---

## 7️⃣ 讨论与局限（Discussion）
- 当前仅考虑**推理**；训练阶段量化策略更复杂（梯度误差累积）。
- SVT 假设“低秩”，若未来出现**非线性交互**强烈的矩阵，需引入**神经网络补全**。
- 未考虑**动态到达 & 抢占**；后续可引入**在线稳定匹配**或**RL 微调**。

---

## 8️⃣ 结论（Conclusion）
> QLLMS 首次把“**量化选型**”与“**边缘调度**”装进**同一优化框架**，在**部分信息**下仍能给出**理论稳定**的匹配结果；实测**成本降 22 %–71 %**，**边缘完成率升 58 %**，为“大模型下沉边缘”提供了**可落地、可证明、可扩展**的一站式方案。

---

## 🌟 核心创新点（Highlights）

1. **联合优化**：“量化比特 × GPU 型号 × 任务”三维决策，而非两阶段割裂。
2. **部分信息补全**：利用**低秩矩阵恢复**+SVT，把缺失 50 % 的 profiling 格子**无损还原**（理论保证）。
3. **稳定匹配**：首次把 LLM 调度建模为**多对一稳定匹配**，证明**无阻塞对**，算法**二次方**复杂度适合边缘实时运行。
4. **实测碾压**：在**真实 Llama/Mistral + 商用 GPU 价格**下，**成本降 22 %–71 %**，**任务边缘完成率升 58 %**。
5. **开源友好**：模块化设计（Profiler/Reconstructer/Scheduler），可直接嵌入**KubeEdge / OpenYurt**；代码与数据集即将开源。

---

# 📽️ 文章全景解读 | MEC 赋能大型视觉模型（LVM）服务的联合推理与卸载优化  
&gt; 会议：IEEE INFOCOM 2025  
&gt; 标题：Joint Optimization of Model Inferencing and Task Offloading for MEC-Empowered Large Vision Model Services  
&gt; 作者：Xinyi Zhuang 等（哈工大深圳）  
&gt; 关键词：大型视觉模型、扩散 Transformer、MEC、MAPPO、SLSQP、视频质量度量、任务卸载

---

## 1️⃣ 引言（Introduction）——为什么“云原生 LVM”不再够用？

| 痛点 | 具体表现 |
|---|---|
| **算力饕餮** | 一段 5 s 512×512 视频需 50–80 步扩散推理，A100 上仍需分钟级延迟。 |
| **带宽饥渴** | 中间潜变量+最终视频可达数百 MB，回传用户耗时 &gt; 推理本身。 |
| **体验敏感** | 自动驾驶、机器人直播等场景要求**秒级 TTFT** 与**十秒级端到端**；云端集中式服务 SLA 难保证。 |

- 作者提出 **MEC-empowered LVM 服务架构**：
  - 云中心（CS）托管**全量模型**，负责“粗推理”前几步；
  - 基站边缘（ES）部署**轻量 LVM**，接收中间潜变量后“精推理”剩余步；
  - 用户（LU）通过无线链路**上传提示词**、**下载生成视频**。
- 核心挑战：
  1. **模型选型+推理步数**如何与用户需求对齐？
  2. **多少步卸载到边缘？**（卸载粒度=单步）
  3. **无线/有线/计算资源**如何联合分配？

→ 首次将“**模型推理**”与“**任务卸载**”**在同一框架内联合优化**，而非传统两阶段割裂。

---

## 2️⃣ 系统模型（System Model）——“云-边-端”长什么样？

### 2.1 网络拓扑
- 1 个云中心 CS + M 个基站边缘 ES + N 个移动用户 LU；
- ES→CS 通过**光纤专线**（10 Gb/s 级），ES→LU 通过**OFDM 无线下行**；
- 每个 ES 绑定**特定 LVM**（例如 Open-Sora Plan v1.1.0），CS 拥有**全量模型仓库**。

### 2.2 扩散 Transformer 计算模型（FLOPs 级建模）
- 采用**时空分离 Transformer**（STB + TTB）：
  - STB：空间自注意力，计算量 ∝ n_f·n_h·n_w·d²；
  - TTB：时间自注意力，计算量 ∝ n²_f·n_h·n_w·d；
- **逐层 FLOPs 解析式**给出，可精准预测**任意步数**的延迟与能耗；
- 中间潜变量大小 = 2×n_f·n_h·n_w·d_hidden，用于**云-边传输**成本估算。

### 2.3 多维度视频质量度量（MD-VQM）
- **提示-视频对齐度 U^A**：用 ViT-L/14 CLIP 计算**文本-帧特征**余弦相似度；
- **经典视频质量 U^Q**：
  - 运动平滑度（MAE 插帧误差）；
  - 时序一致性（帧间 CLIP 相似度）；
  - 客观美学（DOVER 评分）；
- **合并公式**：U_n,m = U^A_n,m · U^Q_n,m(o_n)  
  其中 U^Q 采用**修正指数** 1−e^(−ζ·o_n) 拟合，** diminishing return** 与实测一致。
- **实测 750 段视频**验证：不同 prompt 对 U^A 影响显著，而 U^Q 随推理步数 o_n 单调递增→**步数越多≠越好**，需**折中**。

---

## 3️⃣ 问题形式化（Problem Formulation）——一个 MINLP 的“大杂烩”

**决策变量**：
- x_n,m ∈ {0,1}　　用户 n 是否选择边缘 m 的 LVM  
- o_n ∈ [o_min, o_max]　总推理步数  
- c_n ∈ [c_min, c_max]　在云端执行的步数  
- q_n = o_n − c_n　卸载到边缘的步数  
- v^CS_n, v^ES_n,m　云/边 GPU SM 分配数  
- b_n,m, p_n,m　下行带宽与发射功率

**目标**：
max Σ(ω1·U_n − ω2·T_n − ω3·E_n)  
即**效用最大**，**延迟+能耗最小**。

**约束**：
- 单用户只能绑定一个 ES；
- 云/边 GPU SM 总数上限；
- 基站带宽、功率上限；
- 扩散模型 FLOPs→延迟→闭环表达式；

→ 问题为**混合整数非线性规划（MINLP）**，变量耦合严重，无法单步求解。

---

## 4️⃣ 算法设计（Algorithm Design）——“学习+优化”双轮驱动 TS-MITO

### 4.1 分解思路
- **Stage-1 用户侧（Sub-Problem I）**：离散变量 x, o, c　→　**MAPPO** 多智能体强化学习；
- **Stage-2 服务器侧（Sub-Problem II）**：连续变量 v, b, p　→　**凸优化+SLSQP** 解析/数值求解；
- 两阶段**交替迭代**，实现**去中心化**决策。

### 4.2 Stage-1：MAPPO 细节
- **智能体**：每个 LU 一个 actor；全局 critic 统一训练；
- **观测**：信道增益 g_n,m、ES 剩余 SM、prompt 对齐度 U^A；
- **动作**：⟨x_n,m, o_n, c_n⟩ 连续化后 clip；
- **奖励**：即时 weighted utility − delay − energy；
- **技巧**：
  - GAE 估计优势函数；
  - PPO-clip 防止策略震荡；
  - 中心化训练→分布式执行，**&lt;10 ms** 决策时延。

### 4.3 Stage-2：三子问题全解析
- **II-A 云端 GPU 分配** → **凸问题**，KKT 给出**闭式解**：
  v^CS_n ∝ √(α^(1)_n + α^(2)_n)  
- **II-B 边缘 GPU 分配** → **凸问题**，同样闭式解；
- **II-C 无线带宽+功率** → **非凸**，采用 **SLSQP** 局部最优，实测 **&lt;50 ms** 收敛。

### 4.4 TS-MITO 完整流程（Algorithm 1）
1. 每时隙 LU 用 actor 网络选 ⟨模型, 步数, 卸载步⟩；
2. CS/ES 按闭式解切 GPU SM；
3. ES 用 SLSQP 分配 b, p；
4. 环境返回奖励，存储轨迹；
5. 每 episode 结束更新 actor & critic。

→ **二次方复杂度**，适合**毫秒级**边缘控制。

---

## 5️⃣ 仿真结果（Simulation Results）——数字说话

### 5.1 设置
- 15 LU, 3 ES, 1 CS；Open-Sora 参数按论文表 I；
- 对比基准：IPPO、VDN、QMIX、Equal/Proportional 分配；
- 指标：service utility、delay、energy、收敛速度。

### 5.2 关键数字
| 指标 | TS-MITO 提升（vs 次优） |
|---|---|
| **延迟降低** | 17.2 %（vs QMIX） |
| **能耗降低** | 21.7 %（vs QMIX） |
| **效用提升** | 3 %（绝对值 12.91→13.30） |
| **收敛速度** | 700 episode 稳定，比 IPPO 快 **76.6 %** |

### 5.3 泛化实验
- **不同分配策略**：Equal/Proportional 下奖励↑17.7 %/8.8 %；
- **不同权重因子**：ω1↑→效用↑，延迟&能耗同步↑，算法**自适应**；
- **不同 LVM 强度**：Best→Worst 场景，效用差 **22 %**，TS-MITO **始终最优**。

---

## 6️⃣ 结论与未来工作（Conclusion & Future Work）

&gt; 本文首次将“**模型推理步数决策**”与“**逐步卸载**”纳入同一框架，提出**数据驱动的 MD-VQM 度量**与**学习-优化混合 TS-MITO 算法**，在真实扩散 Transformer 参数下实现**17 % 延迟↓、21 % 能耗↓、3 % 效用↑**，并具备**理论收敛保证**与**毫秒级实时性**。

- **未来方向**：
  1. **在线步长调整**：推理过程中动态决定是否继续精化；
  2. **多 LVM 协作**：边缘模型间流水线或集成生成；
  3. **训练-推理混合**：边缘微调+推理一体化调度；
  4. **视频语义缓存**：热门 prompt/潜变量边缘缓存，进一步削减云侧负载。

---

## 🌟 核心创新点（Highlights）

1. **MD-VQM**：首个**实测驱动**的 LVM 视频质量度量，把“prompt-对齐”与“经典画质”乘性融合，**可拟合、可微、可泛化**到新模型。
2. **步级卸载**：不再“整模型”或“整任务”卸载，而是**以单步扩散为粒度**决定“云做几步-边做几步”，**带宽与计算双节省**。
3. **TS-MITO 两阶段架构**：
   - MAPPO 解决**离散决策**（选模型、选步数、选卸载步）；
   - 闭式解+SLSQP 解决**连续资源**（GPU SM、带宽、功率）；
   - **去中心化执行**，**理论稳定**，**&lt;50 ms** 完成一次调度。
4. **真实模型验证**：基于开源 **Open-Sora Plan v1.1.0** 完整 FLOPs 参数，**750 段视频实测拟合**，实验结果**可复现、可部署**。
5. **性能碾压**：在相同硬件成本下，**延迟↓17 %、能耗↓21 %、效用↑3 %**，全面优于 IPPO/VDN/QMIX 等 SOTA 多智能体算法。

---

## 📌 一句话总结
&gt; TS-MITO 让“大型视觉模型”在**云-边协同**环境中**像水龙头一样可调**：想快就云端少几步，想精就边缘多几步，**质量、延迟、能耗**三旋钮实时最优，为 6G 时代“**生成式视频即服务**”提供了**可落地、可证明、可扩展**的调度范式。