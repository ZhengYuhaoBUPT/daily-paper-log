# 摘要：LEO星座遥感推理的微服务鲁棒强化学习部署框架  
&gt; 原文：Microservice Deployment in Space Computing Power Networks via Robust Reinforcement Learning  
&gt; 作者：Zhiyong Yu 等 | IEEE TMC 2025（已录用，作者稿）

---

## 1 章节速览

| 章节 | 核心内容 |
|---|---|
| **I. 引言** | 遥感数据下传→星上计算需求；单体推理"高延迟+资源异构"→微服务架构；提出"鲁棒强化学习"解决 NP-hard 部署问题。 |
| **II. 系统模型** | ① 微服务 DAG（轻量/核心两类）&lt;br&gt;② Walker-Star LEO 星座拓扑（ISL+四象限）&lt;br&gt;③ 延迟模型：传输+传播+处理&lt;br&gt;④ 成本模型：部署/维护/并行 4 项&lt;br&gt;⑤ 约束：功能完整、并行上限、QoS、资源上限&lt;br&gt;⑥ 确定性整数规划（目标：最小化总成本） |
| **III. 鲁棒部署问题** | 引入"箱式不确定集"刻画区域请求波动；两阶段鲁棒优化（min-max-min）；QoS 约束→半无限；证明传统求解器失效。 |
| **IV. 鲁棒对抗强化学习** | ① 问题分解：Stage-1 静态核心服务（标准 RL）+ Stage-2 轻量服务（鲁棒对抗 RL，POMDP）&lt;br&gt;② MSRARL 算法：主智能体 vs 扰动智能体交替训练，PPO 更新&lt;br&gt;③ 奖励 = 部署成功+资源惩罚+QoS 惩罚&lt;br&gt;④ 理论：将博弈转化为二人零和，证明 Minimax=Nash 均衡存在 |
| **V. 仿真** | 6-200 颗卫星、2-5 个微服务、10-30 时隙；对比 Vanilla RL、K8s-HPA、鲁棒 HPA；指标：成功请求数、部署数、总延迟、运行时间。 |
| **VI. 结论与未来方向** | 框架在"资源消耗-QoS 惩罚"权衡上优于基线；未来：① 延迟直接融入目标 ② 动态拓扑+能耗 ③ 模型切分 ④ 时空演化不确定集 |

---

## 2 主要创新点（Highlights）

1. **首个面向 LEO 星座的"鲁棒强化学习微服务部署"框架**  
   将星上计算、ISL 多跳、资源异构、请求不确定四重因素统一建模。

2. **两阶段分解 + 对抗训练机制**  
   - Stage-1：标准 RL 快速锁定"核心"微服务（部署一次，长期不变）  
   - Stage-2：MSRARL 让"扰动智能体"生成最坏请求，主智能体在线调整轻量微服务，实现"训练-攻击"闭环。

3. **半无限 QoS 约束的 POMDP 转化**  
   通过 reward 塑形把时延违约次数直接量化成惩罚，避免显式求解无限约束。

4. **理论保障**  
   证明 adversarial game 可转化为二人零和矩阵博弈，存在 Minimax=Nash 均衡，为 RL 近似最优解提供依据。

5. **实验验证**  
   - 相同不确定度下，成功请求率 ↑15-25%，总延迟 ↓10-18%  
   - 200 卫星规模下单步决策 &lt;100 ms，满足星上实时重部署需求。

---

## 3 一句话总结
&gt; 用"鲁棒对抗强化学习"把 NP-hard 的星上微服务部署问题拆成两段打博弈，既敢对未知请求"留后手"，又能把资源用在刀刃上，实测比 K8s  heuristic 和 vanilla RL 都更省、更快、更稳。

论文目前止步于算法级仿真，K8s 仅作为“策略对照”出现，尚未在真实或半实物 K8s 环境中验证。



# 📡 文章速览 | Hierarchical RL for Task Scheduling in Space-Air Integrated Edge Computing  
&gt; 原文：IEEE IoT-J 2025（已录用）  
&gt; 作者：Yuting Wei 等  
&gt; 场景：LEO 卫星 + UAV 集群服务海上用户，目标：低时延、低能耗的任务调度

---

## 1️⃣ 章节导读

| 章节 | 一句话摘要 |
|---|---|
| **I. 引言** | SAGIN 6G 场景下，UAV 机动高、卫星算力强但资源固定→亟需“空-天”协同调度；传统集中式方法难实时，提出分层 RL 框架。 |
| **II. 相关研究** | 梳理 LP、凸近似、博弈、元启发式→无法适应高动态；单/多智能体 DRL 状态空间爆炸→收敛难。 |
| **III. 系统模型** | 准静态时隙：8 架 UAV 生成 50-100 KB 可分割任务→本地 or 卸载至 3 颗 LEO；建模通信（UAV↔卫星、ISL）与计算（队列+CPU cycle）时延/能耗。 |
| **IV. 算法设计** | 双层 HRL：① UAV 层 MATD3（多智能体）决定本地/卸载比例；② 卫星层 TD3 决定如何把卸载任务二次分发给合作星；共享奖励保证跨层一致。 |
| **V. 仿真评估** | Python+PyTorch；对比 4 种基线（HTT/HDT/NHM/NHS）；指标 ATC、ATD、AEC；HMT 算法 ATC ↓13%+，收敛快、单步测试 10 ms 级。 |
| **VI. 结论** | 首次在空-天边缘用“分层-多智能体-连续动作”RL 解决任务调度；未来加入安全约束、动态 UAV 轨迹、通信开销优化。 |

---

## 2️⃣ 核心创新点（Highlights）

1. **双层解耦架构**  
   - 空层（UAV）（MATD3）→ 决定“做多少本地、卸载多少”  
   - 天层（卫星）（TD3）→ 决定“卸载任务如何二次拆分给合作星”  
   降低单智能体状态-动作空间，收敛速度↑30%。

2. **共享奖励机制**  
   两层奖励函数都包含“全局时延 + 全局能耗”加权，避免“各自最优、系统次优”。

3. **连续动作 + 多智能体**  
   相比离散 DQN / 单智能体 DDPG，MATD3 支持“任意 0-1 卸载比例”与 UAV 间协同，TD3 解决卫星连续分配比例，精度更高。

4. **实测性能领先**  
   - ATC 平均降低 ≥13%，最大 22%（vs NHM）  
   - 单步推理 10 ms，适合星上实时重调度  
   - 资源敏感性实验：中心星 3→6 GHz 时，HMT 曲线最平缓，鲁棒性最好。

5. **开源友好**  
   纯 Python/PyTorch 实现，网络参数、奖励权重全部表格化，易于在 K8s-edge、KubeEdge 等实际平台落地。

---

## 3️⃣ 一句话总结
&gt; 把“空-天”任务调度拆成两级连续控制问题，用 MATD3+TD3 打组合拳，既让 UAV 群自主决定“做不做”，也让卫星群智能“分活”，仿真显示省时、省电、收敛快，为 6G SAGIN 边缘计算提供了可扩展的 RL 范式。